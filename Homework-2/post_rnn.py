# -*- coding: utf-8 -*-
"""POST_Rnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FU8BgJqkYICqSsV-tVZwJClPhIBd0Ijk
"""

# install necessary packages using pip
!pip install keras numpy wget

# To access data from drive
from google.colab import drive
drive.mount('/content/drive')
import os
import sys

def load_corpus(path):

    # Check if the path is a directory.
    if not os.path.isdir(path):
        sys.exit("Input path is not a directory")
    
    # TODO: Your code goes here
    sentences = []
    for filename in os.listdir(path):
        with open(os.path.join(path,filename), "r") as f:
            lines = f.readlines()
            for line in lines:
                temp = line.split()
                sentence = []
                for inp in temp:
                    pair = inp.split('/')
                    token, POS = pair[0], pair[1]
                    sentence.append((token,POS))
                if sentence:
                    sentences.append(sentence)
    return sentences

# test the function here:
path = "/content/drive/MyDrive/modified_brown"
data = load_corpus(path)

import numpy as np # you may need this to convert lists to np arrays before returning them

# Creates the dataset with train_X (words) and train_y (tag).
def create_dataset(sentences):
    # Defines the relevant lists.
    train_X, train_y = list(), list()   
    # TODO: Your code goes here
    word_arr = {"[PAD]":0, "[UNK]":1}
    tag_arr = {"[UNK]":0}
    #get unique words and tags
    for sentence in sentences:
        words, tags = zip(*sentence)
        for i in range(len(words)):
            if words[i] not in word_arr:
                word_arr[words[i]] = len(word_arr)
        for j in range(len(tags)):
            if tags[j] not in tag_arr:
                tag_arr[tags[j]] = len(tag_arr)
    
    #words and tags to integers
    for sentence in sentences:
        word_idxs, tag_idxs = [], []
        for word,tag in sentence:
            word_idxs.append(word_arr[word])
            tag_idxs.append(tag_arr[tag])
        train_X.append(word_idxs)
        train_y.append(tag_idxs)

    train_X = np.array(train_X)
    train_y = np.array(train_y)
    return word_arr, tag_arr, train_X, train_y

# Test the function here
# Call create_dataset()
Vocabulary, Tags, train_X, train_y = create_dataset(data)

from tensorflow.keras.preprocessing.sequence import pad_sequences as pad
# Pad the sequences with 0s to the max length.
def pad_sequences(train_X, train_y):
    # MAX_LENGTH to record length of longest sequence 
    MAX_LENGTH = max(len(np_arr) for np_arr in train_X)
    train_X = pad(train_X, maxlen=MAX_LENGTH, padding='post', value=0)
    train_y = pad(train_y, maxlen=MAX_LENGTH, padding='post', value=0)
    return train_X, train_y, MAX_LENGTH

# Test the function
train_X, train_y, MAX_LENGTH = pad_sequences(train_X, train_y)

from keras.models import Sequential
from keras.layers import InputLayer, Activation
from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding
from keras.optimizers import Adam

# Define the Keras model.
def define_model(vocabulary, tags, MAX_LENGTH):  
    # Define 'model' here
    model = Sequential()
    vocabulary_size = len(vocabulary)
    num_tags = len(tags)
    embedding_vector_size = 128
    Embedding_layer = Embedding(input_dim=vocabulary_size, output_dim=embedding_vector_size, input_length=MAX_LENGTH)
    LSTM_layer = Bidirectional(LSTM(256, return_sequences=True))
    Dense_layer = TimeDistributed(Dense(units=num_tags, activation='softmax'))
    # Adding layers to model
    model.add(Embedding_layer)
    model.add(LSTM_layer)
    model.add(Dense_layer)
    print (model.summary())
    return model

# Call the function here
model = define_model(Vocabulary, Tags, MAX_LENGTH)

# Returns the one-hot encoding of the sequence.
from keras.utils import to_categorical
def one_hot_encode(train_y, num_tags):
    # Using keras built in utility
    train_y = to_categorical(train_y, num_classes=num_tags)
    return train_y

num_tags = len(Tags)
train_y = one_hot_encode(train_y, num_tags)

import tensorflow as tf

# Trains the model.
def train(model, train_X, train_y):

    # Fit the data into the Keras model, through 40 passes (epochs) using model.fit()
    Optimize = Adam(learning_rate=0.001)
    model.compile(loss='categorical_crossentropy', optimizer=Optimize, metrics=['accuracy'])
    model_history = model.fit(train_X, train_y,batch_size=128,epochs=40,validation_split=0.2)
    # Return the model history, model trained objext is at "model".
    return model_history

# call function here
history = train(model, train_X, train_y)

from tensorflow.keras.preprocessing.sequence import pad_sequences as pad
# Test a sentence using the given model.
# Since Tags is dict with TAG(string value) and its INT, Swapping the key value pairs is needed to get the TAG string from the predicted INT.
Tags = {val: key for key, val in Tags.items()}

def test(model, sentence):
    # TODO: Write your code here
    test_str = sentence
    sentence = sentence.split(' ')
    len_sentence = len(sentence)
    sentence_arr = []
    for word in sentence:
        if word in Vocabulary:
            sentence_arr.append(Vocabulary[word])
        else:
            sentence_arr.append(Vocabulary["[UNK]"])
    padded_sentence = pad([sentence_arr], maxlen=MAX_LENGTH, padding='post')
    predicted_logits = model.predict(padded_sentence)[0]
    predicted_tags_int = np.argmax(predicted_logits, axis=-1)
    predicted_tags = []
    for tags_int in predicted_tags_int:
        if tags_int in Tags:
            predicted_tags.append(Tags[tags_int])
        else:
            predicted_tags.append(Tags["[UNK]"])
    #print("The predicted tags for the sentence:", test_str)
    return list(zip(sentence, predicted_tags[:len_sentence]))


# Evaulaiton Senetences, PART-3
testString1 = "the secretariat is expected to race tomorrow ."
testString2 = "people continue to enquire the reason for the race for outer space ."
testString3 = "the planet jupiter and its moons are in effect a mini solar system ."
testString4 = "computers process programs accurately ."

output1 = test(model, testString1)
output2 = test(model, testString2)
output3 = test(model, testString3)
output4 = test(model, testString4)

print(f"The predicted tags for: {testString1}\n{output1}")
print(f"The predicted tags for: {testString2}\n{output2}")
print(f"The predicted tags for: {testString3}\n{output3}")
print(f"The predicted tags for: {testString4}\n{output4}")

################## END ##################